\part{机器学习和深度学习基础}

\chapter{基本概念}

\section{激活函数}

激活函数是神经网络中最典型的非线性层。如果没有激活函数，无论神经网络有多少层，输
出总为输入的线性函数，即可以将多层等效为单个线性层，将无法解决异或这样简单的非线
性问题。

\subsection{Sigmoid 函数}\label{subsec:Sigmoid}

Sigmoid 函数及其导数的形式为：
\begin{align}
  \label{equ:sigmoid}
  \sigma(x) & = \frac{1}{1 + e^{-x}} \\
  \label{equ:sigmoid-d}
  \sigma'(x) & = \sigma(x) (1-\sigma(x))
\end{align}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/基本概念/Sigmoid.png}
  \caption{Sigmoid 函数及其导数的图像}\label{fig:sigmoid}
\end{figure}

\begin{itemize}
  \item 图~\ref{fig:sigmoid}~中蓝色曲线为 sigmoid 函数，其形状与字母 S 类似，可以
    将 $-\infty$ 到 $\infty$ 的输入单调映射到 0 到 1 之间。
  \item 图~\ref{fig:sigmoid}~中红色曲线为 sigmoid 函数的导数，在输入值较大或较小时，
    该值趋于 0，因此 sigmoid 函数作为激活函数时，可能出现梯度消失，网络无法正常训
    练。
\end{itemize}

\subsubsection{线性整流函数（ReLU）}

线性整流函数（Rectifier Linear Unit，ReLU）是目前应用最广泛的激活函数，其形式为：
\begin{equation}
  \label{equ:ReLU}
  \mathrm{ReLU}(x) = \left\{
    \begin{array}{lr}
      x & x > 0 \\
      0 & x \leq 0 \\
    \end{array}
  \right.
\end{equation}

ReLU 的图像如图~\ref{fig:ReLU}~所示。当 $x > 0$ 时，梯度始终为 1；当 $x \leq 0$
时梯度始终为 0，同样会导致梯度消失。

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{images/机器学习和深度学习基础/基本概念/ReLU.png}
  \caption{ReLU 的图像}\label{fig:ReLU}
\end{figure}

\section{损失函数}

\subsection{交叉熵损失函数}\label{subsec:CELoss}

交叉熵损失函数（Cross Entropy Loss，CE Loss）的形式为：
\begin{equation}
  \label{equ:CELoss}
  \mathrm{L}_{\mathrm{CE}} = - \left( y \, \log \, \hat{y} + (1-y) \log (1-\hat{y}) \right )
\end{equation}

其中 $y$ 为真实 label，正样本为 1，负样本为 0，$\hat{y}$ 为预测值，介于 0 和 1
之间，预测值越接近真实 label，则 loss 越小。

\subsection{Softmax 损失函数}

\subsection{$L_1$ 损失函数}

\subsection{$L_2$ 损失函数}

\subsection{Smooth $L_1$ 损失函数}
Smooth $L_1$ 损失函数是 $L_1$ 和 $L_2$ 损失函数的组合，其形式为：
\begin{equation}
  \label{equ:SmoothL1}
  \mathrm{SL}_1(x) = \left\{
    \begin{array}{lr}
      |x| & |x| > \alpha \\
      \frac{1}{\alpha} x^2 & |x| \leq \alpha \\
    \end{array}
  \right.
\end{equation}

由上式可知，Smooth $L_1$ 损失函数在 $x$ 绝对值较小时为 $L_2$ 函数，而在 $x$ 绝对
值较大时为 $L_1$ 函数。在目标检测的两阶段方法中，回归 loss 一般使用 Smooth $L_1$
损失函数。

\subsection{Focal loss}\label{subsec:focal}

文献\citerb{2017-RetinaNet}提出 focal loss，用于解决目标检测中正负样本不平衡的问
题。Focal loss 降低了简单样本的权重，与难样本挖掘（Hard Negative Mining，HEM）类
似。Focal loss 的形式和图像如公式~\ref{equ:focal-loss}~和图~\ref{fig:focal-loss}~所
示：
\begin{equation}
  \label{equ:focal-loss}
  \mathrm{FL}(p_{\mathrm{t}}) = - \alpha_t {(1-p_{\mathrm{t}})}^{\gamma} \log (p_{\mathrm{t}})
\end{equation}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/基本概念/Focal-Loss.pdf}
  \caption{Focal loss 的图像（$\alpha_t = 1$）}\label{fig:focal-loss}
\end{figure}

图~\ref{fig:focal-loss}~中，$\gamma = 0$ 的蓝色曲线即传统的交叉熵 loss。其他颜色
的曲线分别表示 $\gamma$ 取不同数值时的 focal loss。Focal loss 中两个参数的作用
是：

\begin{itemize}
  \item $\alpha_t$：平衡正负样本的权重，一般正负样本对应的 $\alpha_t$ 之和为 1。
  \item $\gamma$：平衡难易样本的权重，$\gamma$ 越大，简单样本的权重越小。例
    如 $\gamma = 2$ 时，对于预测值 $p = 0.9$ 的样本，focal loss 的值要比交叉
    熵 loss 的值小 100 倍。
\end{itemize}

\section{优化方法}\label{sec:opt}

\subsection{梯度下降（GD）和随机梯度下降（SGD）}
梯度下降方法属于一阶优化算法，即利用函数的一阶导数。由于梯度是函数值上升最快的方
向，因此在最小化 loss 时，选择负梯度方向更新参数：
\begin{equation}
  \boldsymbol{\theta}_t = \boldsymbol{\theta}_{t-1} - \eta \cdot \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})
\end{equation}

根据上式中梯度计算时使用样本数的不同，梯度下降法又可分为：
\begin{itemize}
  \item 批梯度下降：使用全部样本计算梯度。
  \item 随机梯度下降：使用单个样本计算梯度。
  \item Mini-batch 梯度下降：使用部分样本（一般为 2 的整数次幂，例如 16、32、64
    等）计算梯度。目前应用最为广泛，有时也称为随机梯度下降，注意与单个样本的随机
    梯度下降做区分。
\end{itemize}

\subsection{带动量的随机梯度下降}
带动量的随机梯度下降可以使迭代过程中满足梯度方向相同时加速，梯度改变方向时减速：
\begin{align}
  v_t & = \gamma v_{t-1} + \eta \cdot \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}) \\
  \boldsymbol{\theta}_{t} & = \boldsymbol{\theta}_{t-1} - v_t
\end{align}

\subsection{Nesterov 方法}
Nesterov 方法是对带动量的随机梯度下降的改进，计算梯度时不用 $\theta$ 而用
$\theta - \gamma_{t-1}$:
\begin{align}
  v_t & = \gamma v_{t-1} + \eta \cdot \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}-\gamma v_{t-1}) \\
  \boldsymbol{\theta}_{t} & = \boldsymbol{\theta}_{t-1} - v_t
\end{align}

\subsection{Adagrad 方法}
Adagrad 是对随机梯度下降的改进，将参数对应梯度除以历史梯度累积和的开方：
\begin{equation}
  \boldsymbol{\theta}_{t} = \boldsymbol{\theta}_{t-1} - \frac{\eta \cdot \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})}{G_t + \epsilon}
\end{equation}

\subsection{Adadelta 方法}
Adagrad 方法中，分母上的历史梯度累计和会越来越大，导致当前更新值很小。Adadelta
方法将历史梯度值的计算改为类似 Nesterov 的形式：
\begin{align}
  g_t & = \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t}) \\
  {E[g^2]}_t & = \gamma {E[g^2]} {t-1} + (1-\gamma)g_t^2 \\
  \boldsymbol{\theta}_{t} & = \boldsymbol{\theta}_{t-1} - \frac{\eta \cdot g_t}{{E[g^2]}_t+\epsilon}
\end{align}

RMSProp 方法和 Adadelta 方法类似。

\subsection{Adam 方法}
Adam 方法在 Adagrad 的基础上，同时计算梯度的动量：
\begin{align}
  m_t & = \beta_1 m_{t-1} + (1-\beta_1) g_t \\
  v_t & = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
  \hat{m}_t & = \frac{m_t}{1-\beta_1^t} \\
  \hat{v}_t & = \frac{v_t}{1-\beta_2^t} \\
  \boldsymbol{\theta}_{t} & = \boldsymbol{\theta_{t-1}} - \frac{\eta}{\sqrt{\hat{v}_t + \epsilon}}\hat{m}_t
\end{align}

\section{反向传播算法}
神经网络的训练是通过不断调整网络参数从而减小损失函数的过程。对网络参数的优化一般
采用基于梯度的方法，因此需要先计算损失函数 $L$ 对神经网络中各参数 $\omega$ 的导数。
反向传播算法利用链式法则，由后向前逐层计算 $L$ 对各层参数 $W$ 的导数，是目前应用
最多的导数计算方法。

考虑一个神经网络中的某个全连接层 $Y = XW + b$，设网络的 loss 为 $L$，根据链式法
则，有如下关系~\citerb{2017-FC-BP}：

\begin{align}
  \label{equ:bp-fc}
  \frac{\partial L}{\partial X} & = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial X} = \frac{\partial L}{\partial Y} W^{\mathrm{T}} \\
  \frac{\partial L}{\partial W} & = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial W} = X^{\mathrm{T}} \frac{\partial L}{\partial Y}\\
  \frac{\partial L}{\partial b} & = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial b} = \frac{\partial L}{\partial Y}
\end{align}

上面三个式子中，$\frac{\partial L}{\partial X}$ 是为了继续进行反向传播计算 loss
对上一层参数的导数；$\frac{\partial L}{\partial W}$ 和 $\frac{\partial
  L}{\partial b}$ 即更新参数时需要的梯度。类似以上过程，从最后的 loss 开始逐层向
后计算 loss 对每一层输入 $X$ 和参数 $W, \, b$ 的梯度。

\subsection{梯度消失和梯度爆炸}\label{subsec:gradient-vanish-explosion}
根据链式法则，对于深层神经网络，浅层参数的梯度与之后所有层输出对输入梯度的乘积
相关。直观理解，如果梯度都小于 1，相乘后的值很小，会导致梯度消失；反之如果梯度都大
于 1，相乘后的值很大，会导致梯度爆炸。

梯度消失的一个典型原因是激活函数使用了 sigmoid 函数。梯度爆炸的一个简单的解决方法
是梯度裁剪（Gradient Clipping），即设定一个阈值，当梯度值大于阈值时，使其等于阈值，
这样即可避免梯度过大。

\section{归一化}

\subsection{批归一化（BN）}\label{sub:BN}

批归一化（Batch Normalization，BN）是文献\citerb{2015-BN}中提出的一种归一化方法，
BN 的作用包括：

\begin{enumerate}
  \item 降低深层神经网络的训练难度。加入 BN 后，训练时可以使用更大的学习率。
  \item 防止过拟合。BN 一定程度上相当于引入了正则项，加入 BN 后可以去掉 dropout
    层，\hyperref[subsec:YOLOv2]{YOLOv2} 就是一个典型的例子。
\end{enumerate}

文献\citerb{2015-BN}中认为 BN 有效的原因是其减少了神经网络的内部协同变化
（Internal Covariate Shift，ICS），但文献\citerb{2018-BN-help-opt}论证 BN 并不能
减少 ICS，其提升性能的原因是使网络的 loss 变得更平滑。

\paragraph{BN 的形式}
BN 包括归一化和线性变换两部分。其中归一化是为了减少上层网络参数变化对输入的影响；
线性变换部分是为了增加 BN 层的拟合能力，如果没有线性变换部分，BN 层的输出将始终为
均值为 0，方差为 1 的分布。BN 的具体形式为：
\begin{align}
  \label{equ:BN}
  \mu_{\mathcal{B}} & = \frac{1}{m} \sum_{i=1}^{m} x_i \\
  \sigma_{\mathcal{B}}^2 & = \frac{1}{m} \sum_{i=1}^{m} {(x_i-\mu_{\mathcal{B}})}^2 \\
  \hat{x}_i & = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} \\
  y_i & = \gamma \hat{x}_i + \beta
\end{align}

\begin{itemize}
\item 上式中 $m$ 为一个 batch 的样本数，即期望和方差计算的都是一个 batch 的统计量。
  训练时采用滑动平均法计算均值和方差，即同时考虑历史 batch 和当前 batch 的统计量，
  例如 Caffe 中的~\href{https://bit.ly/2JBY7aw}{BN层}，测试时\textbf{使用训练时保
    存的均值和方差}，不需要都重新计算每个 batch 的统计量。
  \item BN 层中有两个需要学习的参数，即线性变换的参数 $\gamma$ 和 $\beta$，这两
    个参数的初值分别为 1 和 0。
\end{itemize}

\paragraph{CNN 中的 BN}
\begin{itemize}
  \item 卷积层之后 BN 均值和方差的计算方法：计算的是一个 batch 中 $m$ 个样本
    同一个 feature map 上所有点的均值和方差。如果一个 batch 对应的卷积层的输出维度
    为 $m \times C \times H \times W$，则只计算通道数 $C$ 个均值和方差，不同
    feature map 利用与其对应的均值和方差进行归一化。
  \item BN 的位置：文献~\citerb{2015-BN}~中给出的结构是\textbf{卷积层-BN-ReLU}，
    即 BN 层加在卷积层之后，ReLU 之前。
  \item 卷积层的偏置 $b$：由于 BN 中包括减均值，因此可以省略卷积层的偏置。
  \item 测试时将卷积层和 BN 层进行合并：由于卷积层和 BN 层都是线性变换，且测试
    时 BN 层使用的均值和方差均为训练时保存的结果，因此可以将二者合并以提升计算速
    度，合并后参数的计算方法可以参考文献\citerb{2017-DSSD}。
\end{itemize}

\subsection{组归一化（GN）}\label{sub:GN}

文献\citerb{2018-GN}中提出了组归一化（Group Normalization，GN）方法，主要解决 BN
在 batch 较小时性能不佳的问题。

\paragraph{GN 的形式}
不同归一化方法的对比如图~\ref{fig:all-norm}~所示：
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/基本概念/GN.pdf}
  \caption{不同归一化方法对比}\label{fig:all-norm}
\end{figure}

\begin{itemize}
  \item BN：对一个 batch 内不同样本的同一个 feature map 做归一化，共有通道
    数 $C$ 个均值和方差。
  \item LN：同一样本，对所有通道的 feature map 做归一化，共有 batch 样本数 $N$
    个均值和方差。
  \item GN：同一样本，对按组划分通道的 feature map 做归一化，共有 batch 样本数乘
    以组数 $N \times G$ 个均值和方差。一般 $G$ 取 32，LN 可看做 GN 中 $G=1$ 的特
    例。
\end{itemize}

\chapter{卷积神经网络}

\section{卷积类型}

\subsection{分组卷积}

分组卷积（Group Convolution，GC）在 AlexNet 中就已经应用，但当时的原因是 GPU 计
算能力不足，只能将标准卷积拆成 2 个分组卷积~\citerb{2012-AlexNet}。分组卷积和标准
卷积的对比如图~\ref{fig:normal-conv} 和图~\ref{fig:group-conv} 所
示~\citerb{2019-Diff-Conv}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/卷积神经网络/标准卷积.png}
  \caption{标准卷积}\label{fig:normal-conv}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/卷积神经网络/分组卷积.png}
  \caption{分组卷积}\label{fig:group-conv}
\end{figure}

分组卷积的具体步骤是：

\begin{enumerate}
  \item 将输入按通道拆分成 $g$ 组，每组数据的尺寸与输入相同，均为 $H_{\mathrm{in}}
    \times W_{\mathrm{in}}$，通道数均为输入的 $ 1/g $，即 $
    C_{\mathrm{g}} = C_{\mathrm{in}} / g $。
  \item 分别对拆分后的 $g$ 组数据进行标准卷积，每一组数据对应的输出的通道数
    均为 $ C_{\mathrm{out}}/g $。
  \item 将 $g$ 组数据的输出结果进行拼接，得到通道数为 $C_{\mathrm{out}}$
    的输出。
\end{enumerate}

分组卷积可以节省计算量，因此在轻量型网络中得到广泛应用。参数相同的情况下，分组卷
积的计算量约为标准卷积的 $ 1 / g^2 $，组数越多，计算量越小。

分层卷积（Depthwise Convolution，DW）是分组卷积的一种特例，是指组数 $g$ 与输入数
据通道数 $C_{\mathrm{in}}$ 相同的卷积，即每组卷积的输入只有 1 个通道。

\subsection{深度可分离卷积}\label{subsec:DSC}

深度可分离卷积（Depthwise Separable Convolution，DSC）如图~\ref{fig:ds-conv}~所
示~\citerb{2019-Diff-Conv}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/卷积神经网络/深度可分离卷积.png}
  \caption{深度可分离卷积}\label{fig:ds-conv}
\end{figure}

深度可分离卷积包括分层卷积和逐点卷积两部分，计算量为标准卷积计算量的 $ 1 /
C_{\mathrm{in}} + 1/k^2 $，$k$ 为分层卷积的卷积核大小。
当通道数 $C_{\mathrm{in}}$ 较大时，上式可近似为 $1 /
k^2$~\citerb{2017-MobileNet-v1}。

\subsection{反卷积}\label{subsec:deconv}
反卷积（Deconvolution），有时也称转置卷积（Transposed Convolution），是一种特殊的卷积
形式。反卷积的具体步骤是，先通过补 0 扩大输入图像的尺寸，再进行标准卷积。反卷积的
作用是将卷积后尺寸较小的 feature map 恢复到与输入大小相同的尺寸。 需要特别说明的
是，反卷积虽然可以恢复尺寸，\textbf{但不能保证恢复后的数值与输入相同}~\citerb{2016-Guide-Conv}。

\section{感受野}

感受野（Receptive Field，RF） 是指卷积神经网络中某一层的一个像素，会受到多少个输
入数据像素的影响。直观上理解，就是该像素能看到多大范围的输入层像
素~\citerb{2017-Guide-to-RF-cal}。具体的计算方法为：
\begin{align}
\label{equ:rf-cal}
j_{\mathrm {out}} & = j_{\mathrm{in}} \times s \\
r_{\mathrm {out}} & = r_{\mathrm{in}} + (k-1) \times j_{\mathrm{in}}
\end{align}

其中 $j$ 为 jump，即每一层的 stride；s 为该层运算的 stride，k 为卷积的 kernel
size，r 即感受野。利用公式~\eqref{equ:rf-cal}~即可计算各层的感受野。经典网络 VGG-16
各层感受野的详细计算可以参考文献~\citerb{2018-VGG-16-RF-cal}。

%%% Local Variables:
%%% TeX-master: "../master"
%%% End:
