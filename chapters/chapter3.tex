\part{机器学习和深度学习基础}

\chapter{基本概念}

\section{激活函数}

激活函数的作用是引入非线性因素，提升神经网络的拟合能力。如果没有激活函数，无论神
经网络有多少层，输出总为输入的线性函数，因此网络的拟合能力将会十分有限。

\subsection{Sigmoid 函数}
\label{subsec:Sigmoid}

Sigmoid 函数的原函数和导数为：
\begin{align}
  \label{equ:sigmoid}
  \sigma(x) & = \frac{1}{1 + e^{-x}} \\
  \label{equ:sigmoid-d}
  \sigma'(x) & = \sigma(x) (1-\sigma(x))
\end{align}

Sigmoid 函数的图像如图~\ref{fig:sigmoid}~所示：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/基本概念/Sigmoid.png}
  \caption{Sigmoid 函数及其导数的图像}
  \label{fig:sigmoid}
\end{figure}

由图~\ref{fig:sigmoid}~中的蓝色曲线可知，Sigmoid 函数的形状与字母 S 类似，可以
将 $-\infty$ 到 $\infty$ 的输入单调映射到 0 到 1 之间。由图~\ref{fig:sigmoid}~中
的红色曲线可知，在输入值较大或较小时，Sigmoid 函数的导数趋近于 0，这个性质可能导
致网络出现~\ref{subsec:gradient-vanish-explosion}~小节提到的梯度消失问题，从而导
致网络无法正常训练。

\subsection{线性整流函数(ReLU)}

线性整流函数（Rectifier Linear Unit）是目前应用广泛的激活函数，其形式为：

\begin{equation}
  \label{equ:ReLU}
  \mathrm{ReLU}(x) = \left\{
    \begin{array}{lr}
      x & x > 0 \\
      0 & x \leq 0 \\
    \end{array}
  \right.
\end{equation}

ReLU 的图像很简单，如图~\ref{fig:ReLU}~所示。由图中可看出，ReLU 在 $x > 0$ 时对
应的导数始终为 1，不会出现梯度消失现象；但当 $x \leq 0$ 时梯度始终为 0，同样会导
致梯度消失。

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{images/机器学习和深度学习基础/基本概念/ReLU.png}
  \caption{ReLU 的图像}
  \label{fig:ReLU}
\end{figure}

\section{损失函数}

\subsection{交叉熵损失函数}

\subsection{$L_1$ 损失函数}

\subsection{$L_2$ 损失函数}

\subsection{Smooth $L_1$ 损失函数}
Smooth L1 是 $L_1$ 和 $L_2$ 损失函数的组合，其形式为：
\begin{equation}
  \label{equ:SmoothL1}
  \mathrm{SL}_1(x) = \left\{
    \begin{array}{lr}
      |x| & |x| > \alpha \\
      \frac{1}{\alpha} x^2 & |x| \leq \alpha \\
    \end{array}
  \right.
\end{equation}

由上式可知，Smooth $L_1$ 在 $x$ 的绝对值较小时为 $L_2$ 函数，而在 $x$ 的绝对值较
大时为 $L_1$ 函数。R-CNN 系列文章中，回归 loss 均使用 Smooth $L_1$ 形
式\citerb{2015-Fast-RCNN, 2015-Faster-RCNN}。

\subsection{Focal loss}

Focal loss 是由文献\citerb{2017-RetinaNet}中提出，用于解决目标检测中正负样本不平
衡的问题。一般而言，只要样本存在类别不平衡，都可以尝试引入 focal loss 解决，相当
于进行了难样本挖掘（HEM）。

Focal loss 的形式和图像如公式~\ref{equ:focal-loss}~和图~\ref{fig:focal-loss}~所示：
\begin{equation}
  \label{equ:focal-loss}
  \mathrm{FL}(p_{\mathrm{t}}) = - \alpha_t (1-p_{\mathrm{t}})^{\gamma} \mathrm{log}(p_{\mathrm{t}})
\end{equation}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/基本概念/Focal-Loss.pdf}
  \caption{Focal loss 的图像（$\alpha_t = 1$）}
  \label{fig:focal-loss}
\end{figure}

图~\ref{fig:focal-loss}~中，$\gamma = 0$ 的蓝色曲线即传统的交叉熵 loss，可以认为
是 focal loss 的一个特例。其他颜色的曲线分别表示 $\gamma$ 取不同数值时的 focal
loss。Focal loss 中两个参数的具体作用是：

\begin{itemize}
  \item $\alpha_t$：用于平衡不同类别样本的权重，一般所有类别对应的 $\alpha_t$ 之
    和为 1。
  \item $\gamma$：用于平衡难易样本的权重，$\gamma$ 越大，则简单样本的等效权重越
    小。例如 $\gamma = 2$ 时，$p = 0.9$ 的样本，focal loss 的值比交叉熵 loss 的
    值小 100 倍，而 $p = 0.968$ 的样本则小 1000 倍。
\end{itemize}

\section{反向传播算法}
考虑一个神经网络中的某个全连接层 $Y = XW + b$，设网络的 loss 为 $L$，根据链式法
则，有如下关系~\citerb{2017-FC-BP}：

\begin{align}
  \label{equ:bp-fc}
  \frac{\partial L}{\partial X} & = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial X} = \frac{\partial L}{\partial Y} W^{\mathrm{T}} \\
  \frac{\partial L}{\partial W} & = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial W} = X^{\mathrm{T}} \frac{\partial L}{\partial Y}\\
  \frac{\partial L}{\partial b} & = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial b} = \frac{\partial L}{\partial Y}
\end{align}

上面三个式子中，$\frac{\partial L}{\partial X}$ 是为了继续进行反向传播，因为上一
层参数导数的计算依赖该值；$\frac{\partial L}{\partial W}$ 和 $\frac{\partial
  L}{\partial b}$ 即更新参数时需要的梯度。前向计算完成后，逐层向后计算 loss 对每
一层输入($X$)和参数($W, \, b$)的梯度。

\subsection{梯度消失和梯度爆炸}
\label{subsec:gradient-vanish-explosion}
根据链式法则，对于深层神经网络，其浅层参数的梯度与之后所有层输出对输入梯度的乘积
相关。直观理解，如果梯度都小于 1，相乘后的值很小，导致梯度消失；反之如果梯度都大
于 1，相乘后的值很大，导致梯度爆炸。

梯度消失的一个典型原因是使用了 Sigmoid 函数，具体原因请见~\ref{subsec:Sigmoid}小节中的分析。

梯度爆炸的一个简单直接的解决方法是梯度裁剪（Gradient Clipping），即当梯度的值超过
一定阈值时，将其设为阈值的值，避免出现梯度过大的情况。

\section{归一化}

\subsection{批归一化}
\label{sub:BN}

批归一化（Batch Normalization，BN）

\chapter{卷积神经网络}

\section{卷积类型}

\subsection{分组卷积}

分组卷积（Group Convolution，GC）在最早的 AlexNet 中就已经应用，但当时应用的原因是 GPU 计算能力不足，只能
将标准卷积拆成 2 个分组卷积~\citerb{2012-AlexNet}。分组卷积和标准卷积的对比如
图 \ref{fig:normal-conv} 和图 \ref{fig:group-conv} 所示~\citerb{2019-Diff-Conv}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/卷积神经网络/标准卷积.png}
  \caption{标准卷积}
  \label{fig:normal-conv}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/卷积神经网络/分组卷积.png}
  \caption{分组卷积}
  \label{fig:group-conv}
\end{figure}

由以上两图可知，分组卷积的具体步骤是：

\begin{enumerate}
  \item 将输入拆分成 $g$ 组，每组数据的尺寸与输入相同，均为 $H_{\mathrm{in}}
    \times W_{\mathrm{in}}$，但每组的 channel 数均为输入的 $ 1/g $，即 channel
    数为 $ C_{\mathrm{in}} / g $。
  \item 对拆分后的 $g$ 组数据分别进行标准卷积，每一组数据对应的输出的 channel 数
    均为 $ C_{\mathrm{out}}/g $。
  \item 将 $g$ 组数据的输出结果进行 concat，得到 channel 数为 $C_{\mathrm{out}}$
    的输出。
\end{enumerate}

由以上分析可知，与标准卷积相比，分组卷积的计算量约为其 $ 1 / g^2 $，即组数越多，
计算量越小。

分层卷积（Depthwise Convolution，DW）是分组卷积的一种特例，指的是组数 $g$ 与 输
入数据的 channel 数 $C_{\mathrm{in}}$ 相同的卷积，即每组卷积的输入只有 1 个 channel。

\subsection{深度可分离卷积}

深度可分离卷积（Depthwise Separable Convolution，DSC）如图~\ref{fig:ds-conv}~所
示~\citerb{2019-Diff-Conv}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/机器学习和深度学习基础/卷积神经网络/深度可分离卷积.png}
  \caption{深度可分离卷积}
  \label{fig:ds-conv}
\end{figure}

深度可分离卷积的步骤包括分层卷积和逐点卷积两部分，计算量为标准卷积计算量的 $ 1 /
C_{\mathrm{in}} + 1/k^2 $，$k$ 为分层卷积的卷积核大小。
当 channel 数 $C_{\mathrm{in}}$ 较大时，上式可近似为 $1 / k^2$~\citerb{2017-MobileNet-v1}。

\subsection{反卷积}

反卷积(Deconvolution)，有时也称转置卷积(Transposed Convolution)，是一种特殊的卷积
形式。反卷积的具体步骤是，先通过补 0 扩大输入图像的尺寸，再进行标准卷积。反卷积的
作用是将卷积后尺寸较小的 feature map 恢复到与输入大小相同的尺寸。 需要特别说明的
是，反卷积虽然可以恢复尺寸，\textbf{但不能保证恢复后的数值与输入相同}~\citerb{2016-Guide-Conv}。

\section{感受野}

感受野（Receptive Field，RF） 是指卷积神经网络中某一层的一个像素，会受到多少个输
入数据像素的影响。直观上理解，就是该像素能看到多大范围的输入层像
素~\citerb{2017-Guide-to-RF-cal}。具体的计算方法为：
\begin{align}
\label{equ:rf-cal}
j_{\mathrm {out}} & = j_{\mathrm{in}} \times s \\
r_{\mathrm {out}} & = r_{\mathrm{in}} + (k-1) \times j_{\mathrm{in}}
\end{align}

其中 $j$ 为 jump，相当于每一层的 stride；s 为卷积的 stride，k 为卷积的 kernel
size，r 即感受野。利用 \eqref{equ:rf-cal} 即可计算各层的感受野。

经典网络 VGG-16 各层感受野的详细计算可以参考~\citerb{2018-VGG-16-RF-cal}。

%%% Local Variables:
%%% TeX-master: "../master"
%%% End:
