\part{生成对抗网络（GAN）}

\chapter{GAN 基础}
\section{GAN}

文献\citerb{2014-GAN}中第一次提出了生成对抗网络（Generative Adversarial
Network，GAN），通过交替训练生成器 G 和判别器 D，生成器 G 可以将输入的噪声 $z$ 转
化为类似训练样本的图片。

\subsection{GAN 的生成器 G 和判别器 D}

\begin{itemize}
  \item 生成器 G：输入为随机的隐空间向量 $z$，输出为与真实数据维度相同的数
    据 $G(z)$。对于图像生成任务，如果输入为 3 通道 RGB 图片，那么生成器 G 同样输
    出 3 通道 RGB 图片（维度为 $3 \times H \times W$）。
  \item 判别器 D：输入为真实数据 $x_{\mathrm{real}}$ 或生成器 G 的输出
    $x_{\mathrm{fake}} = G(z)$，输出为输入是真实数据的概率 $D(x)$。
\end{itemize}

\subsection{GAN 的损失函数}

\begin{itemize}
  \item 判别器 D 的损失函数为\hyperref[subsec:CELoss]{交叉熵损失函数}：
  \begin{equation}
    \label{equ:GAN-D}
    \mathrm{L}_{\mathrm{GAN-D}} = - \left( \mathrm{log} D(x_i) + \mathrm{log} (1-D(G(z))) \right )
  \end{equation}

  \item 生成器 G 的损失函数为判别器 D 损失函数的后一项取相反数，即：
  \begin{equation}
    \label{equ:GAN-G}
    \mathrm{L}_{\mathrm{GAN-G}} =  \mathrm{log} (1-D(G(z)))
  \end{equation}
\end{itemize}

\subsection{GAN 的训练}

GAN 的训练过程为交替训练判别器 D 和生成器 G。

\begin{itemize}
  \item 判别器 D 的训练：输入为 $x$ 和 $G(z)$ 组成的 batch，输出为 $D(x)$ 和
    $D(G(z))$，计算 loss 反传更新 D 的参数。
  \item 生成器 G 的训练：输入为 $z$，输出为 $D(G(z))$，计算 loss 反传更新 G 的参
    数。
\end{itemize}

理想情况下，判别器 D 和生成器 G 收敛到纳什均衡解，即生成器 G 完美还原原始数据，判
别器 D 对任何输入始终输出 0.5。

\section{GAN 的训练技巧}
\subsection{归一化}
\subsubsection{谱归一化}
文献\citerb{2018-SN}提出谱归一化（Spectral Normalization，SN)，将其加入 GAN 的判
别器可以提升 GAN 训练的稳定性。

假设神经网络中的卷积或全连接层为 $y = Wx$，其中 $W$ 为权重矩阵，SN 方法是将权重
$W$ 归一化为 $W/\sigma(W)$，其中 $\sigma(W)$ 为 $W$ 的谱范数，即：
\begin{equation}
  \sigma(W) = \sqrt{\mathrm{max} \, \left | \mathrm{eig} (W^{\mathrm{T}} W) \right |}
\end{equation}

SN 在实际应用时，采用迭代求解的方式以加快计算速度，具体算法如
图~\ref{fig:sn-algo}~所示。

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/GAN/SN.pdf}
  \caption{Spectral Normalization 迭代算法流程}
  \label{fig:sn-algo}
\end{figure}

\subsection{正则化}
\subsubsection{一致性正则化}
文献\citerb{2019-CRGAN}提出一致性正则化（Consistency Regularization，CR），可以
与 SN 一起使用提高 GAN 的性能。CR 的目的是惩罚判别器 D 对真实图片增广变换的敏感性：
\begin{equation}
  L_{\mathrm{CR}} = \left\| D(x) - D(T(x)) \right\|_2
\end{equation}
其中 $T$ 表示增广变换。

文献\citerb{2020-ICR}对 CR 进行改进，提出 ICR，加入判别器 D 对生成器生成图片增广变换敏感性的 loss：
\begin{equation}
  L_{\mathrm{CR\_fake}} = \left\| D(G(z)) - D(T(G(z))) \right\|_2
\end{equation}

此外，ICR 还考虑了判别器 D 对隐空间扰动的一致性：
\begin{equation}
  L_{\mathrm{CR\_z}} = \left\| D(G(z)) - D(G(z+\Delta z)) \right\|_2
\end{equation}

其中 $\Delta z$ 为加到 $z$ 上满足正态分布的噪声。但只加该 loss 容易导致生成器 G
出现 collapse，即对不同的 $z$， G 都生成类似的图像，因此需要在生成器 G 训练时加
入多样性 loss：
\begin{equation}
  L_{\mathrm{G\_z}} = -\left\| D(G(z)) - D(G(z+\Delta z)) \right\|_2
\end{equation}

\section{GAN 的损失函数}
\subsection{原始 GAN 的损失函数}
原始 GAN 使用的损失函数为：
\begin{align}
  \label{equ:origin-GAN-loss}
  \mathrm{L}_{\mathrm{GAN-D}} & = - \left( \mathrm{log} D(x_i) + \mathrm{log} (1-D(G(z))) \right ) \\
  \mathrm{L}_{\mathrm{GAN-G}} & = \mathrm{log} (1-D(G(z)))
\end{align}

原始 GAN 中判别器 D 最后加入了 sigmoid 模块，即 D 的输出在 0 和 1 之间。

\subsection{Hinge Loss 形式的损失函数}
Hinge Loss 形式的损失函数为：
\begin{align}
  \label{equ:Hinge-GAN-loss}
  \mathrm{L}_{\mathrm{D}} & = -\mathrm{min}(0, -1 + D(x)) - \mathrm{min}(0,
  -1-D(G(z))) \\
  \mathrm{L}_{\mathrm{G}} & = -D(G(z))
\end{align}

判别器 D 最后未加入 sigmoid，输出值为 $-\infty$ 到 $\infty$ 之间。

\subsection{Wassertein 形式的损失函数}
Wassertein 形式的损失函数为：
\begin{align}
  \label{equ:WGAN-loss}
  \mathrm{L}_{\mathrm{D}} & = -D(x) + D(G(z)) \\
  \mathrm{L}_{\mathrm{G}} & = D(G(z))
\end{align}

与 hinge loss 形式类似，判别器 D 最后未加入 sigmoid，输出值为 $-\infty$ 到 $\infty$ 之间。

Wassertein 形式损失函数的改进版是在损失函数中加入判别器 D 梯度的正则项，
即 D 的 loss 为：
\begin{equation}
  \label{equ:WGAN-GP-loss}
  \mathrm{L}_{\mathrm{D}} = -D(x) + D(G(z)) + \lambda \left( \left\| \nabla D(x) \right\|_2 - 1 \right ) ^ 2
\end{equation}

%%% Local Variables:
%%% TeX-master: "../master"
%%% End:
