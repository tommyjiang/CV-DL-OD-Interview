\part{分类}

\chapter{经典模型}

\section{SENet/SKNet}
SENet 和 SKNet 的主要思想是利用 feature map 信息调整不同 feature map 的权重。

\subsection{SENet}
\label{subsec:SENet}
SENet 的基本单元如图~\ref{fig:senet}~所示\citerb{2017-SENet}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/SENet.pdf}
  \caption{SENet 基本模块}
  \label{fig:senet}
\end{figure}

图中 sigmoid 之后得到的权重用于与对应 channel 的 feature map 相乘，以调整不
同 feature map 的权重。

\subsection{SKNet}
SKNet 的基本单元如图~\ref{fig:sknet}~所示\citerb{2019-SKNet}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{images/分类/SKNet.png}
  \caption{SKNet 基本模块}
  \label{fig:sknet}
\end{figure}

SKNet 相当于通过网络选择不同卷积核（$3 \times 3$ 和 $5 \times 5$）生成 feature
map 的权重。

\chapter{轻量模型}

\section{MobileNet 系列}

\section{ShuffleNet 系列}
\subsection{ShuffleNet v1}
ShuffleNet v1 的基本单元如图~\ref{fig:shufflenet}~所示：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/ShuffleNet.pdf}
  \caption{ShuffleNet 基本模块}
  \label{fig:shufflenet}
\end{figure}

上图中（b）和（c）分别对应 stride = 1 和 stride = 2 的模块，借鉴了 ResNet 中
Residual block 的结构，具体结构包括：

\begin{itemize}
  \item 卷积模块：$1 \times 1$ 分组卷积 + Shuffle + $3 \times 3$ 分层卷积 + $1
    \time 1$ 分组卷积，分层卷积的 stride 与整体 stride 相同（1 或 2）。
  \item Shortcut 路径：stride = 2 模块为 $3 \times 3$，stride = 2 的平均池化。
  \item 合并方法：stride = 1 模块为 element-wise 求和，stride = 2 模块为 concat。
\end{itemize}

\subsection{ShuffleNet v2}

\chapter{参数搜索模型}

\section{NAS 搜索}

\section{EfficientNet}
EfficientNet 提出综合考虑网络深度（层数）、宽度（通道数）和输入分辨率三个维度的
放大比例，利用网格搜索得到 400M FLOPS 下的小网络 EfficientNet B0，再按照等比例放
大的原则得到一系列网络 EfficientNet B1-B7。

\section{RegNet}

\chapter{涨点 Tricks}
\section{模型改进}
\subsection{ResNet-D}
文献\citerb{2018-BoT}中对 ResNet 结构进行了三点改进：
\begin{itemize}
  \item 将第一层 $7 \times 7, s=2$ 卷积改为三个 $3 \times 3$ 的卷积，其中第一个
    卷积的 stride 为 2。
  \item 将下采样模块的 $1 \times 1, s=2 + 3 \times 3$ 卷积改为 $1 \times 1 + 3
    \times 3, s=2$ 卷积。
  \item 将下采样模块的 $1 \times 1, s=2$ 卷积改为 $2 \times 2$ 平均池化 + $1
    \times 1$ 卷积。
\end{itemize}

\section{正则化}
\subsection{数据增广}
\subsubsection{AutoAugment}
文献\citerb{2018-AutoAugment}中利用强化学习得到适用于不同数据集的最优增广策略，每
个策略包括 5 个子策略，其中每个子策略又包括两个具体的操作：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/AutoAugment.pdf}
  \caption{AutoAugment 方法得到的 ImageNet 数据集的最优策略}
  \label{fig:autoaugment}
\end{figure}


\subsubsection{GridMask}
文献\citerb{2020-GridMask}中将规则的 0-1 mask 覆盖到输入数据，将 mask 值为 0 位置
的输入值设为 0，相当于人为将部分输入数据置为 0。

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/GridMask.pdf}
  \caption{GridMask 方法示例}
  \label{fig:gridmask}
\end{figure}

\subsection{Label 相关}
\subsubsection{Label Smooth Regularization（LSR）}
文献\citerb{2015-inception-v2-v3}中提出 LSR 策略：
\begin{equation}
  q'(k) = (1-\epsilon) \delta_{k,y} + \frac{\epsilon}{K}
\end{equation}

\begin{itemize}
  \item 不采用 LSR 策略时，GT class 的 label 为 1，其他 class 为 0。
  \item 采用 LSR 策略时，GT class 的 label 为 $(1-\epsilon) + \frac{\epsilon}{K}$，
    其他 class 为 $\frac{\epsilon}{K}$。
\end{itemize}

\subsubsection{mixup}
文献\citerb{2017-mixup}中提出 mixup 策略，将两个训练样本融合得到新的训练样本：
\begin{align}
  \begin{split}
    \tilde{x} = & \, \lambda x_i + (1-\lambda) x_j  \\
    \tilde{y} = & \, \lambda y_i + (1-\lambda) y_j  \\
  \end{split}
\end{align}

其中 $x$ 为输入数据，$y$ 为 label。

\subsection{Drop 系列}
\subsubsection{DropBlock}
文献\citerb{2018-DropBlock}提出 DropBlock，可看作改进版的 Dropout，去掉的是连续
区域内的输入，而非 Dropout 中的随机区域。

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/DropBlock.pdf}
  \caption{(c)DropBlock 与 (b)Dropout 对比}
  \label{fig:dropblock}
\end{figure}

\section{训练技巧}


%%% Local Variables:
%%% TeX-master: "../master"
%%% End:
