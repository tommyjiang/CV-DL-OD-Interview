\part{分类}

\chapter{经典模型}

\section{ResNet 系列}

\subsection{ResNet}\label{subsec:ResNet}

\subsection{ResNeSt}

\section{Feature map attention 模型}
SENet 和 SKNet 的主要思想是利用 feature map 信息调整不同 feature map 的权重，相
当于在 feature map 层面加入 attention。

\subsection{SENet}\label{subsec:SENet}
SENet 的基本单元如图~\ref{fig:senet}~所示\citerb{2017-SENet}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/SENet.pdf}
  \caption{SENet 基本模块}\label{fig:senet}
\end{figure}

图中 sigmoid 之后得到的权重用于与对应 channel 的 feature map 相乘，以调整不
同 feature map 的权重。

\subsection{SKNet}
SKNet 的基本单元如图~\ref{fig:sknet}~所示\citerb{2019-SKNet}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{images/分类/SKNet.png}
  \caption{SKNet 基本模块}\label{fig:sknet}
\end{figure}

SKNet 相当于通过网络选择不同卷积核（$3 \times 3$ 和 $5 \times 5$）生成 feature
map 的权重。

\chapter{轻量模型}
\section{SqueezeNet}
SqueezeNet\cite{2016-SqueezeNet} 提出三个缩小网络大小的方法：
\begin{enumerate}
  \item 减小卷积核：用 $1 \times 1$ 卷积代替 $3 \times 3$ 卷积。
  \item 减少卷积输入通道数：用 $1 \times 1$ 卷积的 Squeeze 模块减少卷积输入通道数，
    之后再接同时包含 $1 \times 1$ 和 $3 \times 3$ 卷积的 Expand 模块。
  \item 下采样位置：在网络深层进行下采样。
\end{enumerate}

\section{Xception}
Xception\cite{2016-Xception}
网络利用\hyperref[subsec:DSC]{深度可分离卷积}减少参数和计算量。深度可分离卷积先
用 depthwise 卷积融合空间信息，再用 pointwise 卷积融合通道信息。

\section{MobileNet 系列}
\subsection{MobileNet v1}
MobileNet v1 与 Xception 类似，基本模块同样采用深度可分离卷积，降采样利用
带 stride 的 depthwise 卷积实现，同时引入 width 系数和input resolution 系数，
方便设计不同计算量的模型\citerb{2017-MobileNet-v1}。

\subsection{MobileNet v2}\label{subsec:MobileNet-v2}
MobileNet v2 的基本模块是 inverted residual 模块，后续论文有时也称其 MBConv 模
块。MBConv 模块的结构与 ResNet 中的 residual 模块类似，也有一些区
别\citerb{2018-MobileNet-v2}。

\begin{enumerate}
  \item 减少非线性模块：MBConv 的最后一个卷积后面不加非线性模块。
  \item 通道数关系：MBConv 的通道数是中间多两边少，而 residual 模块是中间少两边多。
    这也是 inverted residual 名称的来源，即中间和两边的通道数关系与 residual 模块
    相反。
  \item 中间层卷积类型：MBConv 中间卷积层是 depthwise 卷积，residual 模块中间卷积
    层是普通卷积。
  \item 非线性模块：MBConv 中的非线性模块使用 ReLU6，即 ReLU 后再加一个最大值为
    6 的限制，residual 模块直接使用 ReLU。
\end{enumerate}

MnasNet 同样使用 inverted residual 作为基本模块，利用 NAS 搜索卷积核、扩
张 ratio、stride 等参数\citerb{2018-MnasNet}。

\subsection{MobileNet v3}
MobileNet v3 改进了 MobileNet v2 中的 inverted residual 模块\citerb{2019-MobileNet-v3}：

\begin{enumerate}
  \item 加入 SE 模块：在 inverted residual 模块的 depthwise 卷积结果上加入 SE 模块。
  \item 改进非线性模块：用 h-swish 模块代替 ReLU，h-swish 模块的形式为
    \begin{equation}
      \operatorname{h-swish}(x) = x \frac{\mathrm{ReLU}6 (x+3)}{6}
    \end{equation}
\end{enumerate}

MobileNet v3 中并不是所有模块都加入 SE 和使用 h-swish，具体结构由 NAS 搜索得到。

\section{ShuffleNet 系列}
\subsection{ShuffleNet v1}
ShuffleNet v1 的基本单元如图~\ref{fig:shufflenet}~所示\citerb{2017-ShuffleNet}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/ShuffleNet.pdf}
  \caption{ShuffleNet 基本模块}\label{fig:shufflenet}
\end{figure}

上图中（b）和（c）分别对应 stride = 1 和 stride = 2 的模块，借鉴了 ResNet 中
Residual block 的结构，具体结构包括：

\begin{itemize}
  \item 卷积模块：$1 \times 1$ 分组卷积 + Shuffle + $3 \times 3$ 分层卷积 + $1
    \times 1$ 分组卷积，分层卷积的 stride 与整体 stride 相同（1 或 2）。注意只有第一
    个卷积后有 ReLU，后两个卷积之后没有。
  \item Shortcut 路径：stride = 2 模块为 $3 \times 3$，stride = 2 的平均池化。
  \item 合并方法：stride = 1 模块为 element-wise 求和，stride = 2 模块为 concat。
\end{itemize}

\subsection{ShuffleNet v2}
ShuffleNet v2 总结了轻量神经网络设计的四个基本原则\citerb{2018-ShuffleNet-v2}。

\begin{enumerate}
  \item 输入输出通道数尽可能相同。
  \item 尽量减少分组卷积。
  \item 尽量减少模块中的单独操作，提高并行能力。
  \item 尽量减少 element-wise 操作。
\end{enumerate}

ShuffleNet v2 中的基本模块如~\ref{fig:shufflenet-v2}~所示：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/ShuffleNet-v2.pdf}
  \caption{ShuffleNet v2 基本模块}\label{fig:shufflenet-v2}
\end{figure}

和 ShuffleNet v1 相比，ShuffleNet v2 去掉了分组卷积模块，将 element-wise 相加改
为 concat，同时将 shuffle 操作从 block 内部挪到了两个 block 之间，减少了模块内部
操作。

\section{GhostNet}
GhostNet 没有沿用 MobileNet v2 中的 inverted residual 模块，改为 $1 \times 1$ 卷
积 +（Identity + $3 \times 3$ depthwise 卷积）的结构，称为 Ghost 模块，
如图~\ref{fig:ghostnet}~所示\citerb{2019-GhostNet}：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{images/分类/GhostNet.pdf}
  \caption{GhostNet 中的 Ghost 模块和 Ghost Block}\label{fig:ghostnet}
\end{figure}

借鉴 ResNet 中的 bottleneck 结构，可以将两个 Ghost block 堆叠得到 Ghost
bottleneck。注意 Add 之前的 ghost module 后面只有 BN，没有 ReLU。

\chapter{参数搜索模型}

\section{NAS 搜索}

\section{EfficientNet}
EfficientNet 提出综合考虑网络深度（层数）、宽度（通道数）和输入分辨率三个维度的放
大比例，利用参数网格搜索得到 400M FLOPS 下的小网络 EfficientNet B0，再按照等比例
放大的原则得到一系列网络 EfficientNet
B1-B7~\cite{2019-EfficientNet}。EfficientNet 中的基础模块仍然使
用~\hyperref[subsec:MobileNet-v2]{MobileNet v2} 中的 MBConv 模块。

\section{RegNet}

\chapter{涨点 Tricks}
\section{模型改进}
\subsection{ResNet-D}
文献\citerb{2018-BoT}中对 ResNet 结构进行了三点改进：
\begin{itemize}
  \item 将第一层 $7 \times 7, s=2$ 卷积改为三个 $3 \times 3$ 的卷积，其中第一个
    卷积的 stride 为 2。
  \item 将下采样模块的 $1 \times 1, s=2 + 3 \times 3$ 卷积改为 $1 \times 1 + 3
    \times 3, s=2$ 卷积。
  \item 将下采样模块的 $1 \times 1, s=2$ 卷积改为 $2 \times 2$ 平均池化 + $1
    \times 1$ 卷积。
\end{itemize}

\section{正则化}
\subsection{数据增广}
\subsubsection{AutoAugment}
文献\citerb{2018-AutoAugment}中利用强化学习得到适用于不同数据集的最优增广策略，每
个策略包括 5 个子策略，其中每个子策略又包括两个具体的操作：

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/AutoAugment.pdf}
  \caption{AutoAugment 方法得到的 ImageNet 数据集的最优策略}\label{fig:autoaugment}
\end{figure}


\subsubsection{GridMask}
文献~\citerb{2020-GridMask}中用规则的 0 和 1 mask 覆盖输入数据，将 mask 值为 0 位置
的输入值设为 0，相当于人为将部分输入数据置为 0。

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/GridMask.pdf}
  \caption{GridMask 方法示例}\label{fig:gridmask}
\end{figure}

\subsection{Label 相关}
\subsubsection{Label Smooth Regularization（LSR）}
文献\citerb{2015-inception-v2-v3}中提出 LSR 策略：
\begin{equation}
  q'(k) = (1-\epsilon) \delta_{k,y} + \frac{\epsilon}{K}
\end{equation}

\begin{itemize}
  \item 不采用 LSR 策略时，GT class 的 label 为 1，其他 class 为 0。
  \item 采用 LSR 策略时，GT class 的 label 为 $(1-\epsilon) + \frac{\epsilon}{K}$，
    其他 class 为 $\frac{\epsilon}{K}$。
\end{itemize}

\subsubsection{mixup}
文献\citerb{2017-mixup}中提出 mixup 策略，将两个训练样本融合得到新的训练样本：
\begin{align}
  \begin{split}
    \tilde{x} = & \, \lambda x_i + (1-\lambda) x_j  \\
    \tilde{y} = & \, \lambda y_i + (1-\lambda) y_j  \\
  \end{split}
\end{align}

其中 $x$ 为输入数据，$y$ 为 label。

\subsection{Drop 系列}
\subsubsection{DropBlock}
文献\citerb{2018-DropBlock}提出 DropBlock，可看作改进版的 Dropout，去掉的是连续
区域内的输入，而非 Dropout 中的随机区域。

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{images/分类/DropBlock.pdf}
  \caption{(c) DropBlock 与 (b) Dropout 对比}\label{fig:dropblock}
\end{figure}

\section{训练技巧}


%%% Local Variables:
%%% TeX-master: "../master"
%%% End:
